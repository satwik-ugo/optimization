{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show onnx onnxruntime\n",
    "%pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All required functions and classes\n",
    "#RUN ONCE\n",
    "\n",
    "import torch, onnx\n",
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io, imageio\n",
    "from PIL import Image\n",
    "import time\n",
    "import torchvision\n",
    "import tensorflow.compat.v1 as tf\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "def letterbox(im, new_shape=(416, 416), color=(114, 114, 114), auto=True, scaleup=True, stride=32):    \n",
    "    '''Resize and pad image while meeting stride-multiple constraints.'''\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "    elif isinstance(new_shape, list) and len(new_shape) == 1:\n",
    "        new_shape = (new_shape[0], new_shape[0])\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "\n",
    "    return im, r, (left, top)\n",
    "\n",
    "def process_image(img_src, img_size, stride, half):\n",
    "    '''Process image before image inference.'''\n",
    "    image = letterbox(img_src, img_size, stride=stride)[0]\n",
    "    # Convert\n",
    "    image = image.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    image = torch.from_numpy(np.ascontiguousarray(image))\n",
    "    image = image.half() if half else image.float()  # uint8 to fp16/32\n",
    "    image /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    return image, img_src\n",
    "\n",
    "# with open(test_path, 'rb') as f:\n",
    "#     img_np = np.asarray(Image.open(io.BytesIO(f.read())))\n",
    "def xywh2xyxy(x):\n",
    "    '''Convert boxes with shape [n, 4] from [x, y, w, h] to [x1, y1, x2, y2] where x1y1 is top-left, x2y2=bottom-right.'''\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, agnostic=False, multi_label=False):\n",
    "    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results.\n",
    "    This code is borrowed from: https://github.com/ultralytics/yolov5/blob/47233e1698b89fc437a4fb9463c815e9171be955/utils/general.py#L775\n",
    "    Args:\n",
    "        prediction: (tensor), with shape [N, 5 + num_classes], N is the number of bboxes.\n",
    "        conf_thres: (float) confidence threshold.\n",
    "        iou_thres: (float) iou threshold.\n",
    "        agnostic: (bool), when it is set to True, we do class-independent nms, otherwise, different class would do nms respectively.\n",
    "        multi_label: (bool), when it is set to True, one box can have multi labels, otherwise, one box only huave one label.\n",
    "\n",
    "    Returns:\n",
    "         list of detections, echo item is one tensor with shape (num_boxes, 6), 6 is for [xyxy, conf, cls].\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = prediction.shape[2] - 5  # number of classes\n",
    "    pred_candidates = torch.logical_and(prediction[..., 4] > conf_thres, torch.max(prediction[..., 5:], axis=-1)[0] > conf_thres)  # candidates\n",
    "    # Check the parameters.\n",
    "    assert 0 <= conf_thres <= 1, f'conf_thresh must be in 0.0 to 1.0, however {conf_thres} is provided.'\n",
    "    assert 0 <= iou_thres <= 1, f'iou_thres must be in 0.0 to 1.0, however {iou_thres} is provided.'\n",
    "\n",
    "    # Function settings.\n",
    "    max_det = 50 # maximum detection objects\n",
    "    max_wh = 4096  # maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes put into torchvision.ops.nms()\n",
    "    time_limit = 10.0  # quit the function when nms cost time exceed the limit time.\n",
    "    multi_label &= num_classes > 1  # multiple labels per box\n",
    "\n",
    "    tik = time.time()\n",
    "    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
    "    for img_idx, x in enumerate(prediction):  # image index, image inference\n",
    "        x = x[pred_candidates[img_idx]]  # confidence\n",
    "\n",
    "        # If no box remains, skip the next process.\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # confidence multiply the objectness\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        # print(x[:,:4])\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "        # print(box)\n",
    "        # box = x[:,:4]\n",
    "\n",
    "        # Detections matrix's shape is  (n,6), each row represents (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            box_idx, class_idx = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[box_idx], x[box_idx, class_idx + 5, None], class_idx[:, None].float()), 1)\n",
    "        else:  # Only keep the class with highest scores.\n",
    "            conf, class_idx = x[:, 5:].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, class_idx.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Check shape\n",
    "        num_box = x.shape[0]  # number of boxes\n",
    "        if not num_box:  # no boxes kept.\n",
    "            continue\n",
    "        elif num_box > max_nms:  # excess max boxes' number.\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        class_offset = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + class_offset, x[:, 4]  # boxes (offset by class), scores\n",
    "        keep_box_idx = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if keep_box_idx.shape[0] > max_det:  # limit detections\n",
    "            keep_box_idx = keep_box_idx[:max_det]\n",
    "\n",
    "        output[img_idx] = x[keep_box_idx]\n",
    "        if (time.time() - tik) > time_limit:\n",
    "            print(f'WARNING: NMS cost time exceed the limited {time_limit}s.')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n",
    "def button_candidates(boxes, scores, image):\n",
    "\n",
    "    button_scores = []  # stores the score of each button (confidence)\n",
    "    button_patches = []  # stores the cropped image that encloses the button\n",
    "    button_positions = []  # stores the coordinates of the bounding box on buttons\n",
    "\n",
    "    for box, score in zip(boxes, scores):\n",
    "      x_min = int(box[0])\n",
    "      y_min = int(box[1])\n",
    "      x_max = int(box[2])\n",
    "      y_max = int(box[3])\n",
    "\n",
    "      button_patch = image[y_min: y_max, x_min: x_max]\n",
    "      button_patch = cv2.resize(button_patch, (180, 180))\n",
    "\n",
    "      button_scores.append(score)\n",
    "      button_patches.append(button_patch)\n",
    "      button_positions.append([x_min, y_min, x_max, y_max])\n",
    "    return button_patches, button_positions, button_scores\n",
    "\n",
    "\n",
    "charset = {'0': 0,  '1': 1,  '2': 2,  '3': 3,  '4': 4,  '5': 5,\n",
    "           '6': 6,  '7': 7,  '8': 8,  '9': 9,  'A': 10, 'B': 11,\n",
    "           'C': 12, 'D': 13, 'E': 14, 'F': 15, 'G': 16, 'H': 17,\n",
    "           'I': 18, 'J': 19, 'K': 20, 'L': 21, 'M': 22, 'N': 23,\n",
    "           'O': 24, 'P': 25, 'R': 26, 'S': 27, 'T': 28, 'U': 29,\n",
    "           'V': 30, 'X': 31, 'Z': 32, '<': 33, '>': 34, '(': 35,\n",
    "           ')': 36, '$': 37, '#': 38, '^': 39, 's': 40, '-': 41,\n",
    "           '*': 42, '%': 43, '?': 44, '!': 45, '+': 46} # <nul> = +\n",
    "\n",
    "class CharacterRecognizer:\n",
    "  def __init__(self, graph_path=None, verbose=False):\n",
    "    self.graph_path = graph_path #path to the model which is loaded as a graph\n",
    "    self.session = None\n",
    "    self.input = None\n",
    "    self.output = []\n",
    "    self.class_num = 1\n",
    "    self.verbose = verbose\n",
    "\n",
    "    self.idx_lbl = {} #this is the functionally inverse to charset\n",
    "    for key in charset.keys():\n",
    "      self.idx_lbl[charset[key]] = key\n",
    "    self.init_recognizer()\n",
    "    # print('character recognizer initialized!')\n",
    "\n",
    "  def init_recognizer(self):\n",
    "\n",
    "    # load graph and label map from default folder\n",
    "    if self.graph_path is None:\n",
    "      self.graph_path = './models/ocr_graph.pb'\n",
    "\n",
    "    # check existence of the two files\n",
    "    if not os.path.exists(self.graph_path):\n",
    "      raise IOError('Invalid ocr_graph path! {}'.format(self.graph_path))\n",
    "\n",
    "    # load frozen graph\n",
    "    recognition_graph = tf.Graph()\n",
    "    with recognition_graph.as_default():\n",
    "      od_graph_def = tf.GraphDef()\n",
    "      with tf.gfile.GFile(self.graph_path, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "    self.session = tf.Session(graph=recognition_graph)\n",
    "\n",
    "    # prepare input and output request\n",
    "    self.input = recognition_graph.get_tensor_by_name('ocr_input:0')\n",
    "    # self.output.append(recognition_graph.get_tensor_by_name('chars_logit:0'))\n",
    "    # self.output.append(recognition_graph.get_tensor_by_name('chars_log_prob:0'))\n",
    "    self.output.append(recognition_graph.get_tensor_by_name('predicted_chars:0'))\n",
    "    self.output.append(recognition_graph.get_tensor_by_name('predicted_scores:0'))\n",
    "    # self.output.append(recognition_graph.get_tensor_by_name('predicted_text:0'))\n",
    "\n",
    "\n",
    "  def clear_session(self):\n",
    "    if self.session is not None:\n",
    "      self.session.close()\n",
    "\n",
    "  def predict(self, image_np, draw=False):\n",
    "    assert image_np.shape == (180, 180, 3)\n",
    "    img_in = np.expand_dims(image_np, axis=0)\n",
    "    codes, scores = self.session.run(self.output, feed_dict={self.input: img_in}) #returns codes and scores for each code (single letter)\n",
    "    codes, scores = [np.squeeze(x) for x in [codes, scores]]\n",
    "    # print(len(codes), codes)\n",
    "    score_ave = 0\n",
    "    text = ''\n",
    "    for char, score in zip(codes, scores):\n",
    "      if not self.idx_lbl[char] == '+':\n",
    "        score_ave += score\n",
    "        text += self.idx_lbl[char]\n",
    "    score_ave /= len(text)\n",
    "\n",
    "    if self.verbose:\n",
    "      self.visualize_recognition_result(image_np, text, score_ave)\n",
    "\n",
    "\n",
    "    img_show = self.draw_result(image_np, text, score_ave) if draw else image_np\n",
    "    \n",
    "    # print(f\"text = {text}\")\n",
    "\n",
    "    return text, score_ave, np.array(img_show)\n",
    "\n",
    "  @staticmethod\n",
    "  def visualize_recognition_result(image_np, text, scores):\n",
    "    img_pil = Image.fromarray(image_np)\n",
    "    img_show = ImageDraw.Draw(img_pil)\n",
    "    font = ImageFont.truetype('./Arial.ttf', 100)\n",
    "    img_show.text((45, 60), text=text, font=font, fill=(255, 0, 255))\n",
    "    img_pil.show()\n",
    "\n",
    "  @staticmethod\n",
    "  def draw_result(image_np, text, scores):\n",
    "    img_pil = Image.fromarray(image_np)\n",
    "    img_show = ImageDraw.Draw(img_pil)\n",
    "    font = ImageFont.truetype('./Arial.ttf', 60)\n",
    "    img_show.text((45, 60), text=text, font=font, fill=(255, 0, 255))\n",
    "    return img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, onnx\n",
    "import onnxruntime as ort\n",
    "import cv2,os\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "import time\n",
    "import torchvision\n",
    "recognizer = CharacterRecognizer(verbose=False)\n",
    "test_path = \"test_imgs/1737_jpg.rf.873de08c4c0bb5c1b4faaa0572a48afa.jpg\"\n",
    "\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "for file_name in os.listdir(\"./test_imgs/\"):\n",
    "    test_path = \"./test_imgs/\" + file_name\n",
    "    print(test_path)\n",
    "    img_np = cv2.imread(test_path)\n",
    "    img_n,_ = process_image(img_np,(416,416),2,False)\n",
    "    # print(img_n)\n",
    "    img_n = img_n.numpy()[np.newaxis]\n",
    "\n",
    "    #load model\n",
    "    onnx_model = onnx.load('models/yolov6/best_ckpt.onnx')\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    ort_sess = ort.InferenceSession('models/yolov6/best_ckpt.onnx')\n",
    "    preds = ort_sess.run(None, {'images': img_n})\n",
    "    preds = np.array(preds)\n",
    "    preds = preds.reshape(1,3549,6)\n",
    "    preds = torch.tensor(preds)\n",
    "    dets = non_max_suppression(preds, conf_thres=0.25, iou_thres=0.45, agnostic=False)[0]\n",
    "\n",
    "    dets = dets.tolist()\n",
    "    boxes = [row[:4] for row in dets]\n",
    "    scores = [row[4] for row in dets]\n",
    "\n",
    "    button_patches, button_positions, _ = button_candidates(boxes, scores, img_np)\n",
    "\n",
    "    for button_img in button_patches:\n",
    "        # get button text and button_score for each of the images in button_patches\n",
    "        button_text, button_score, _ = recognizer.predict(button_img)\n",
    "        print(button_text)\n",
    "\n",
    "print(time.time()-st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270 307 316 340\n",
      "95 232 141 264\n",
      "182 307 229 339\n",
      "269 232 316 264\n",
      "270 268 315 302\n",
      "268 64 318 102\n",
      "270 193 316 227\n",
      "183 68 231 102\n",
      "103 135 140 161\n",
      "184 192 232 227\n",
      "96 269 143 302\n",
      "272 135 311 162\n",
      "96 194 142 227\n",
      "97 65 144 99\n",
      "95 306 142 340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "test_path = \"test_imgs/15_jpg.rf.7e4ba4c0c0bdb3beea120118c56fd793.jpg\"\n",
    "img_np = cv2.imread(test_path)\n",
    "img_n,_ = process_image(img_np,(416,416),2,False)\n",
    "img_n = img_n.numpy()[np.newaxis]\n",
    "\n",
    "#load models\n",
    "onnx_model = onnx.load('models/yolov6/best_ckpt.onnx')\n",
    "onnx.checker.check_model(onnx_model)\n",
    "ort_sess = ort.InferenceSession('models/yolov6/best_ckpt.onnx')\n",
    "\n",
    "#get preds\n",
    "preds = ort_sess.run(None, {'images': img_n})\n",
    "preds = np.array(preds)\n",
    "preds = preds.reshape(1,3549,6)\n",
    "preds = torch.tensor(preds)\n",
    "dets = non_max_suppression(preds, conf_thres=0.25, iou_thres=0.45, agnostic=False)[0]\n",
    "dets = dets.tolist()\n",
    "boxes = [row[:4] for row in dets]\n",
    "scores = [row[4] for row in dets]\n",
    "\n",
    "recognizer = CharacterRecognizer(verbose=False)\n",
    "button_patches, button_positions, _ = button_candidates(boxes, scores, img_np)\n",
    "\n",
    "for button_img, button_pos in zip(button_patches, button_positions):\n",
    "        button_text, button_score, button_draw =recognizer.predict(button_img, draw=True)\n",
    "        x_min, y_min, x_max, y_max = button_pos\n",
    "        print(x_min, y_min, x_max, y_max)\n",
    "        button_rec = cv2.resize(button_draw, (x_max-x_min, y_max-y_min))\n",
    "        img_np[y_min+6:y_max-6, x_min+6:x_max-6] = button_rec[6:-6, 6:-6]\n",
    "\n",
    "cv2.imshow('IMage',img_np)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.126 ðŸš€ Python-3.9.16 torch-2.0.1+cu117 CPU\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from models/yolov8/saved_models/yolov8nano.pt with input shape (1, 3, 416, 416) BCHW and output shape(s) (1, 5, 3549) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.7s, saved as models/yolov8/saved_models/yolov8nano.onnx (11.6 MB)\n",
      "\n",
      "Export complete (1.0s)\n",
      "Results saved to \u001b[1m/home/satarw/optimization/models/yolov8/saved_models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models/yolov8/saved_models/yolov8nano.onnx imgsz=416 \n",
      "Validate:        yolo val task=detect model=models/yolov8/saved_models/yolov8nano.onnx imgsz=416 data=/content/datasets/Elevator-buttons-2/data.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models/yolov8/saved_models/yolov8nano.onnx'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert yolov8 to onnx\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('models/yolov8/saved_models/yolov8nano.pt')  # load model\n",
    "\n",
    "# Export the model\n",
    "model.export(format='onnx',imgsz=416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer from yolov8onnx+ocr\n",
    "import os\n",
    "test_path = \"test_imgs/15_jpg.rf.7e4ba4c0c0bdb3beea120118c56fd793.jpg\"\n",
    "img_np = cv2.imread(test_path)\n",
    "img_n,_ = process_image(img_np,(416,416),2,False)\n",
    "img_n = img_n.numpy()[np.newaxis]\n",
    "\n",
    "#load models\n",
    "onnx_model = onnx.load('models/yolov8/yolov8nano.onnx')\n",
    "onnx.checker.check_model(onnx_model)\n",
    "ort_sess = ort.InferenceSession('models/yolov8/yolov8nano.onnx')\n",
    "\n",
    "#get preds\n",
    "preds = ort_sess.run(None, {'images': img_n})\n",
    "preds = np.array(preds)\n",
    "preds = preds.reshape(1,5,3549)\n",
    "npreds = np.zeros((1, 1, 6, 3549))\n",
    "npreds[:, :, :5, :] = preds\n",
    "preds = torch.tensor(npreds)\n",
    "print(preds)\n",
    "dets = non_max_suppression(preds, conf_thres=0.25, iou_thres=0.45, agnostic=False)[0]\n",
    "dets = dets.tolist()\n",
    "boxes = [row[:4] for row in dets]\n",
    "print(boxes)\n",
    "scores = [row[4] for row in dets]\n",
    "\n",
    "recognizer = CharacterRecognizer(verbose=False)\n",
    "button_patches, button_positions, _ = button_candidates(boxes, scores, img_np)\n",
    "\n",
    "for button_img, button_pos in zip(button_patches, button_positions):\n",
    "        button_text, button_score, button_draw =recognizer.predict(button_img, draw=True)\n",
    "        x_min, y_min, x_max, y_max = button_pos\n",
    "        print(x_min, y_min, x_max, y_max)\n",
    "        button_rec = cv2.resize(button_draw, (x_max-x_min, y_max-y_min))\n",
    "        img_np[y_min+6:y_max-6, x_min+6:x_max-6] = button_rec[6:-6, 6:-6]\n",
    "\n",
    "cv2.imshow('IMage',img_np)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
